{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Transform, and Load raw data into database\n",
    "Assumes a format following this template:\n",
    "\n",
    "*Location Date*\n",
    "\n",
    "*Employer Name*\n",
    "\n",
    "*Employer Location*\n",
    "\n",
    "*Application Title*\n",
    "\n",
    "*Application Contents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import packages: Operating System Interface, System-specific parameters and functions, \n",
    "# Regular expression operations, SQLite3 database, CSV and JSON capability, \n",
    "# Unix style pathname pattern expansion, Core tools for working with streams, \n",
    "# Statistics from text functions, Date-parsin and manipulation, Natural Language Toolkit\n",
    "import os, sys, re, sqlite3, csv, json, glob, io\n",
    "from textstat.textstat import textstat\n",
    "import dateutil.parser as dparser\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function for limiting the length of a string\n",
    "def cap(s, l):\n",
    "    return s if len(s)<=l else s[0:l-3]+'...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a database named 'Applications.db', with table named 'applications'.\n",
    "# Commit table (notably, structure) to database, and close the connection\n",
    "try:\n",
    "\tdb = sqlite3.connect('Applications.db')\n",
    "\tcursor = db.cursor()\n",
    "\tcursor.execute('''\n",
    "\tCREATE TABLE `applications` (\n",
    "\t\t`ID`\tINTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "\t\t`path`\tTEXT,\n",
    "\t\t`language`\tTEXT,\n",
    "\t\t`date`\tTEXT,\n",
    "\t\t`longest_sentence`\tTEXT,\n",
    "\t\t`shortest_sentence`\tTEXT,\n",
    "\t\t`lines`\tINTEGER,\n",
    "\t\t`blanklines`\tINTEGER,\n",
    "\t\t`sentences`\tINTEGER,\n",
    "\t\t`number_words`\tINTEGER,\n",
    "\t\t`avg_wordsize`\tINTEGER,\n",
    "\t\t`common_words`\tBLOB,\n",
    "\t\t`syllable_count`\tINTEGER,\n",
    "\t\t`lexicon_count`\tINTEGER,\n",
    "\t\t`sentence_count`\tINTEGER,\n",
    "\t\t`flesch_reading_ease`\tINTEGER,\n",
    "\t\t`flesch_kincaid_grade`\tINTEGER,\n",
    "\t\t`gunning_fog`\tINTEGER,\n",
    "\t\t`smog_index`\tINTEGER,\n",
    "\t\t`automated_readability_index`\tINTEGER,\n",
    "\t\t`coleman_liau_index`\tINTEGER,\n",
    "\t\t`linsear_write_formula`\tINTEGER,\n",
    "\t\t`dale_chall_readability_score`\tINTEGER,\n",
    "\t\t`readability_consensus`\tTEXT\n",
    "\t);\n",
    "\t''')\n",
    "\tdb.commit()\n",
    "\tdb.close()\n",
    "except Exception as e:\n",
    "\tdb.rollback()\n",
    "\traise e\n",
    "finally:\n",
    "\tdb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data/Eng/AcademicMinds - Ole Vik.txt ...\n",
      "Saved with ID: 1\n",
      "Saving data/Eng/Aon - Ole Vik.txt ...\n",
      "Saved with ID: 2\n",
      "Saving data/Eng/Aspire - Ole Vik.txt ...\n",
      "Saved with ID: 3\n",
      "Saving data/Eng/CBRE - Ole Vik.txt ...\n",
      "Saved with ID: 4\n",
      "Saving data/Eng/Crone Corkill - Ole Vik.txt ...\n",
      "Saved with ID: 5\n",
      "Saving data/Eng/Euromonitor - Ole Vik.txt ...\n",
      "Saved with ID: 6\n",
      "Saving data/Eng/Experis - Ole Vik.txt ...\n",
      "Saved with ID: 7\n",
      "Saving data/Eng/Norwegian Barents Secretariat - Ole Vik.txt ...\n",
      "Saved with ID: 8\n",
      "Saving data/Eng/Proctor & Gamble - Ole Vik.txt ...\n",
      "Saved with ID: 9\n",
      "Saving data/Eng/Rand, Research Assistant - Ole Vik.txt ...\n",
      "Saved with ID: 10\n",
      "Saving data/Eng/SAGE - Ole Vik.txt ...\n",
      "Saved with ID: 11\n",
      "Saving data/Eng/Springer - Ole Vik.txt ...\n",
      "Saved with ID: 12\n",
      "Saving data/Eng/UiO, PhD Cover Letter - Ole Vik.txt ...\n",
      "Saved with ID: 13\n",
      "Saving data/Nor/AdmOrg - Ole Vik.txt ...\n",
      "Saved with ID: 14\n",
      "Saving data/Nor/BLD - Ole Vik.txt ...\n",
      "Saved with ID: 15\n",
      "Saving data/Nor/Bussarbeiderforeningen - Ole Vik.txt ...\n",
      "Saved with ID: 16\n",
      "Saving data/Nor/CAS - Ole Vik.txt ...\n",
      "Saved with ID: 17\n",
      "Saving data/Nor/Eksamensvakt, UiB - Ole Vik.txt ...\n",
      "Saved with ID: 18\n",
      "Saving data/Nor/Epinion - Ole Vik.txt ...\n",
      "Saved with ID: 19\n",
      "Saving data/Nor/Europakontoret - Ole Vik.txt ...\n",
      "Saved with ID: 20\n",
      "Saving data/Nor/Europeisk Ungdom - Ole Vik.txt ...\n",
      "Saved with ID: 21\n",
      "Saving data/Nor/Festspillene i Bergen - Ole Vik.txt ...\n",
      "Saved with ID: 22\n",
      "Saving data/Nor/Finance People - Ole Vik.txt ...\n",
      "Saved with ID: 23\n",
      "Saving data/Nor/Finansdepartementet - Ole Vik.txt ...\n",
      "Saved with ID: 24\n",
      "Saving data/Nor/FishGuard - Ole Vik.txt ...\n",
      "Saved with ID: 25\n",
      "Saving data/Nor/Folkeuniversitetet - Ole Vik.txt ...\n",
      "Saved with ID: 26\n",
      "Saving data/Nor/Forskningsadministrativ avdeling - Ole Vik.txt ...\n",
      "Saved with ID: 27\n",
      "Saving data/Nor/Forskningsrådet - Ole Vik.txt ...\n",
      "Saved with ID: 28\n",
      "Saving data/Nor/Førerkort Spesialisten - Ole Vik.txt ...\n",
      "Saved with ID: 29\n",
      "Saving data/Nor/HiOA - Ole Vik.txt ...\n",
      "Saved with ID: 30\n",
      "Saving data/Nor/HiST - Ole Vik.txt ...\n",
      "Saved with ID: 31\n",
      "Saving data/Nor/HiØ - Ole Vik.txt ...\n",
      "Saved with ID: 32\n",
      "Saving data/Nor/HOV - Ole Vik.txt ...\n",
      "Saved with ID: 33\n",
      "Saving data/Nor/Humanistisk Ungdom - Ole Vik.txt ...\n",
      "Saved with ID: 34\n",
      "Saving data/Nor/Kaffelade - Ole Vik.txt ...\n",
      "Saved with ID: 35\n",
      "Saving data/Nor/Kanalbruket - Ole Vik.txt ...\n",
      "Saved with ID: 36\n",
      "Saving data/Nor/Kemax Økonomi Regnskap - Ole Vik.txt ...\n",
      "Saved with ID: 37\n",
      "Saving data/Nor/KHiB - Ole Vik.txt ...\n",
      "Saved with ID: 38\n",
      "Saving data/Nor/KHiO - Ole Vik.txt ...\n",
      "Saved with ID: 39\n",
      "Saving data/Nor/NHO - Ole Vik.txt ...\n",
      "Saved with ID: 40\n",
      "Saving data/Nor/NORAD - Ole Vik.txt ...\n",
      "Saved with ID: 41\n",
      "Saving data/Nor/Norsk Polarinstitutt - Ole Vik.txt ...\n",
      "Saved with ID: 42\n",
      "Saving data/Nor/Pizzabakeren - Ole Vik.txt ...\n",
      "Saved with ID: 43\n",
      "Saving data/Nor/Røde Kors - Ole Vik.txt ...\n",
      "Saved with ID: 44\n",
      "Saving data/Nor/SampolSosiologi - Ole Vik.txt ...\n",
      "Saved with ID: 45\n",
      "Saving data/Nor/Sodexo - Ole Vik.txt ...\n",
      "Saved with ID: 46\n",
      "Saving data/Nor/Sosiologi - Ole Vik.txt ...\n",
      "Saved with ID: 47\n",
      "Saving data/Nor/Statens Vegvesen - Ole Vik.txt ...\n",
      "Saved with ID: 48\n",
      "Saving data/Nor/Statnett - Ole Vik.txt ...\n",
      "Saved with ID: 49\n",
      "Saving data/Nor/UiO - Ole Vik.txt ...\n",
      "Saved with ID: 50\n",
      "Saving data/Nor/Utredningsseksjonen - Ole Vik.txt ...\n",
      "Saved with ID: 51\n",
      "All operations complete.\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This operation should not be ran on large amounts of files,\n",
    "# or indeed very large individual files\n",
    "\n",
    "# Define a location for data, begin folder-traversal\n",
    "data_path = 'data'\n",
    "for dirname, dirnames, filenames in os.walk(data_path):\n",
    "    # Separate English from Norwegian applications (by subfolder: 'Eng' and 'Nor'), and note encoding\n",
    "    for subdirname in dirnames:\n",
    "        language = os.path.join(subdirname)\n",
    "        if language == 'Eng':\n",
    "            enc = 'ASCII'\n",
    "        elif language == 'Nor':\n",
    "            enc = 'ISO-8859-1'\n",
    "        else:\n",
    "            enc = 'UTF-8'\n",
    "        # Traverse .txt files in subfolders\n",
    "        for filename in glob.glob(os.path.join(data_path+'/'+language, '*.txt')):\n",
    "            filename = filename.replace('\\\\', '/')\n",
    "            # For each file, read its contents as 'data' (with line-breaks) and 'string' (without)\n",
    "            with io.open(filename, 'r') as f:\n",
    "                data = f.readlines()\n",
    "                string = ''.join(data)\n",
    "                \n",
    "                # Reset counting mechanisms for each file\n",
    "                lines = 0\n",
    "                blanklines = 0\n",
    "                word_list = []\n",
    "                cf_dict = {}\n",
    "                word_dict = {}\n",
    "                punctuations = [\",\", \".\", \"!\", \"?\", \";\", \":\"]\n",
    "                sentences = 0\n",
    "                date = 0\n",
    "                \n",
    "                # Try to find a date in the first line, !replaced by later function\n",
    "                date = dparser.parse(data[0],fuzzy=True)\n",
    "                # Find sentences and words from 'string'\n",
    "                sentences_tokenized = sent_tokenize(string)\n",
    "                word_count_tokenized = lambda sentences_tokenized: len(word_tokenize(sentences_tokenized))\n",
    "                shortest_sentence = min(sentences_tokenized, key=word_count_tokenized)\n",
    "                longest_sentence = max(sentences_tokenized, key=word_count_tokenized)\n",
    "                shortest_sentence = shortest_sentence.replace('\\n', ' ').replace('\\r', '').replace('  ', ' ')\n",
    "                longest_sentence = longest_sentence.replace('\\n', ' ').replace('\\r', '').replace('  ', ' ')\n",
    "                longest_sentence = cap(longest_sentence, 300)\n",
    "                \n",
    "                # Read content, line by line, to determine amount of lines, words, and sentences\n",
    "                for line in data:\n",
    "                    lines += 1\n",
    "                    if line.startswith('\\n'):\n",
    "                        blanklines += 1\n",
    "                    word_list.extend(line.split())\n",
    "                    for char in line.lower():\n",
    "                        cf_dict[char] = cf_dict.get(char, 0) + 1\n",
    "\n",
    "                for word in word_list:\n",
    "                    lastchar = word[-1]\n",
    "                    if lastchar in punctuations:\n",
    "                        word = word.rstrip(lastchar)\n",
    "                    word = word.lower()\n",
    "                    word_dict[word] = word_dict.get(word, 0) + 1\n",
    "\n",
    "                for key in cf_dict.keys():\n",
    "                    if key in '.!?':\n",
    "                        sentences += cf_dict[key]\n",
    "                \n",
    "                # Count words, average word size, most common words\n",
    "                number_words = len(word_list)\n",
    "                num = float(number_words)\n",
    "                avg_wordsize = len(''.join([k*v for k, v in word_dict.items()]))/num\n",
    "                mcw = sorted([(v, k) for k, v in word_dict.items()], reverse=True)\n",
    "                common_words = json.dumps(mcw, encoding=enc)\n",
    "                \n",
    "                # Remove location from first line, read date with language-sensitive format\n",
    "                raw_date = data[0].strip('Bergen').strip()\n",
    "                if language == 'Eng':\n",
    "                    date = datetime.strptime(raw_date, \"%d/%m/%Y\").date()\n",
    "                elif language == 'Nor':\n",
    "                    date = datetime.strptime(raw_date, \"%d.%m.%Y\").date()\n",
    "                \n",
    "                # Connect to database, attempt to save all data from the current file\n",
    "                try:\n",
    "                    db = sqlite3.connect('Applications.db')\n",
    "                    cursor = db.cursor()\n",
    "                    cursor.execute(\"SELECT ID FROM  applications WHERE path = ?\", (filename.decode(enc),))\n",
    "                    result = cursor.fetchone()\n",
    "                    if result is None:\n",
    "                        print('Saving %s ...'%filename.decode(enc))\n",
    "                        cursor.execute('''INSERT INTO \n",
    "                        applications(path, language, date, longest_sentence, shortest_sentence, lines, blanklines, sentences, number_words, avg_wordsize, common_words, syllable_count, lexicon_count, sentence_count, flesch_reading_ease, flesch_kincaid_grade, gunning_fog, smog_index, automated_readability_index, coleman_liau_index, linsear_write_formula, dale_chall_readability_score, readability_consensus)\n",
    "                        VALUES(:path, :language, :date, :longest_sentence, :shortest_sentence, :lines, :blanklines, :sentences, :number_words, :avg_wordsize, :common_words, :syllable_count, :lexicon_count, :sentence_count, :flesch_reading_ease, :flesch_kincaid_grade, :gunning_fog, :smog_index, :automated_readability_index, :coleman_liau_index, :linsear_write_formula, :dale_chall_readability_score, :readability_consensus)''',\n",
    "                        {'path':filename.decode(enc), 'language':language, 'date':date, 'longest_sentence':longest_sentence, 'shortest_sentence':shortest_sentence, 'lines':lines, 'blanklines':blanklines, 'sentences':sentences, 'number_words':number_words, 'avg_wordsize':avg_wordsize, 'common_words':common_words, 'syllable_count':textstat.syllable_count(string), 'lexicon_count':textstat.lexicon_count(string), 'sentence_count':textstat.sentence_count(string), 'flesch_reading_ease':textstat.flesch_reading_ease(string), 'flesch_kincaid_grade':textstat.flesch_kincaid_grade(string), 'gunning_fog':textstat.gunning_fog(string), 'smog_index':textstat.smog_index(string), 'automated_readability_index':textstat.automated_readability_index(string), 'coleman_liau_index':textstat.coleman_liau_index(string), 'linsear_write_formula':textstat.linsear_write_formula(string), 'dale_chall_readability_score':textstat.dale_chall_readability_score(string), 'readability_consensus':textstat.readability_consensus(string)})\n",
    "                        db.commit()\n",
    "                        id = cursor.lastrowid\n",
    "                        print('Saved with ID: %d' % id)\n",
    "                    else:\n",
    "                        print('%s already exists with ID %s'%(filename.decode(enc),result[0]))\n",
    "                except Exception as e:\n",
    "                    db.rollback()\n",
    "                    raise e\n",
    "                finally:\n",
    "                    db.close()\n",
    "\n",
    "# Confirm that all files found have been evaluated\n",
    "print('All operations complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
